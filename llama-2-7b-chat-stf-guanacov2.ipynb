{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.10/site-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: peft in /root/miniconda3/lib/python3.10/site-packages (0.11.1)\n",
      "Requirement already satisfied: transformers==4.31.0 in /root/miniconda3/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: trl in /root/miniconda3/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (2024.5.15)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.31.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.10/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.27.2)\n",
      "Requirement already satisfied: lit in /root/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /root/miniconda3/lib/python3.10/site-packages (from trl) (0.8.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: bitsandbytes==0.40.0.post4 in /root/miniconda3/lib/python3.10/site-packages (0.40.0.post4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from scipy) (1.25.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch datasets accelerate peft  transformers==4.31.0 trl\n",
    "!pip install bitsandbytes==0.40.0.post4\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #Python忽略代码运行时产生的警告\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"Llama2-7b-chat-hf\"       # 基础模型路径\n",
    "guanaco_dataset = \"guanaco-llama2-1k\"  # 数据集路径\n",
    "new_model = \"llama-2-7b-chat-guanaco\"  # 微调模型名称\n",
    "dataset = load_dataset(guanaco_dataset, split=\"train\") # 加载微调数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compute_dtype = getattr(torch, \"float16\") # 用于线性层计算的数据类型             \n",
    "quant_config = BitsAndBytesConfig(      # 量化参数\n",
    "    load_in_4bit=True,                  # 启用 4 位加载\n",
    "    bnb_4bit_quant_type=\"nf4\",          # 指定用于量化的数据类型nf4 。\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,# 用于线性层计算的数据类型\n",
    "    bnb_4bit_use_double_quant=False,    # 是否使用嵌套量化来提高内存效率\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /root/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(  # 加载基础模型\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    #device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1    # 设置张量并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True) # 加载tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token    # 序列结束的标记eos_token默认是 [SEP]\n",
    "tokenizer.padding_side = \"right\"             # padding_side 设置为right以修复 fp16 的问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peft_params = LoraConfig(   # 指定LoRA微调参数\n",
    "    lora_alpha=16,          # LoRA 缩放因子 \n",
    "    lora_dropout=0.1,       # dropout的比例\n",
    "    r=64,                   # 更新矩阵的秩\n",
    "    bias=\"none\",            # 指定是否应训练 bias 参数。\n",
    "    task_type=\"CAUSAL_LM\",  # 模型任务类型，这里是CAUSAL_LM任务\n",
    ")\n",
    "\n",
    "training_params = TrainingArguments(  # 训练器参数\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    #report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/250 04:19 < 00:54, 0.79 it/s, Epoch 0.82/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.472300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trainer = SFTTrainer(         # 使用SFTTrainer初始化训练器\n",
    "    model=model,              # 指定要训练的模型的路径\n",
    "    train_dataset=dataset,    # 指定用于训练的数据集路径\n",
    "    peft_config=peft_params,  # 指定数据集中用于训练的文本字段\n",
    "    dataset_text_field=\"text\",# 指定数据集中包含文本的字段名称\n",
    "    max_seq_length=None,      # 定义序列的最大长度限制\n",
    "    tokenizer=tokenizer,      # 定义将文本数据转换为模型输入的tokenizer\n",
    "    args=training_params,     # 定义训练过程中的其他设置和参数\n",
    "    packing=False,            # 不将多个权重参数打包成更少的数据单元进行存储和传输\n",
    ")\n",
    "\n",
    "trainer.train()               # 使用trainer训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama-2-7b-chat-guanaco/tokenizer_config.json',\n",
       " 'llama-2-7b-chat-guanaco/special_tokens_map.json',\n",
       " 'llama-2-7b-chat-guanaco/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('llama-2-7b-chat-guanaco/tokenizer_config.json',\n",
       " 'llama-2-7b-chat-guanaco/special_tokens_map.json',\n",
       " 'llama-2-7b-chat-guanaco/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在FP16中重新加载模型并将其与LoRA权重合并\n",
    "load_model = AutoModelForCausalLM.from_pretrained( \n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(load_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "#重新加载tokenizer以保存它\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "model.save_pretrained(new_model)    #保存模型到本地\n",
    "tokenizer.save_pretrained(new_model)#保存tokenizer到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.79s/it]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST] Leonardo da Vinci was a famous Italian artist, inventor, engineer, and scientist.Љ Leonardo was born in 1452 in the village of Vinci in the Tuscany region of Italy. He began his career as an artist in the studio of Andrea del Verrocchio, where he trained in painting, sculpture, and engineering. He later became an independent artist and inventor, and his works include the famous painting \"Mona Lisa\" and many other inventions and designs. Leonardo da Vinci is considered one of the greatest artists of all time, and his legacy continues to inspire people around the world.  [INST] How did he die? [/INST] Leonardo da Vinci died on May 2, 1519, at the age of 67. He was buried in the Chapel of Saint-Hubert in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is Leonardo Da Vinci?\"   #使用新模型进行文本生成\n",
    "pipe = pipeline(task=\"text-generation\", model=new_model, tokenizer=new_model, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is Datacamp Career track? [/INST] DataCamp Career Track is a program that offers a structured learning path and career support to help you land your dream job in data science. Earn a verified certificate and get career support from our team. \n",
      "\n",
      "DataCamp Career Track is designed to help you master the skills you need to succeed in data science, with a focus on practical applications and real-world projects. Our curriculum is built around the skills that are most in demand in the industry, and we offer a range of courses and specializations to help you focus on the areas that interest you most. \n",
      "\n",
      "With DataCamp Career Track, you'll have access to a range of resources and support to help you succeed in your career. Our team of career coaches will work with you one-on-one to help you identify your strengths and weaknesses, set career goals, and develop a plan to achieve\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is Datacamp Career track?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#python查看包版本并输出到txt\n",
    "!pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push model to the hf hub\n",
    "\n",
    "#model.push_to_hub(new_model, use_temp_dir=False)\n",
    "#tokenizer.push_to_hub(new_model, use_temp_dir=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SFT训练环境：官方版:PyTorch/2.0.1/3.10(ubuntu22.04)/12.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###bitsandbytes相关报错 libbitsandbytes_cuda122.so不存在\n",
    "##cuda 122 - not compatible with current bitsandbbytes #551\n",
    "#https://github.com/TimDettmers/bitsandbytes/issues/551\n",
    "#https://github.com/TimDettmers/bitsandbytes/issues\n",
    "#bitsandbytes 0.40.0.post4 includes libbitsandbytes_cuda122.so so I believe this ticket should be closed.\n",
    "\n",
    "\n",
    "#Ubuntu18.04+CUDA11安装bitsandbytes出现的问题\n",
    "#https://blog.csdn.net/steptoward/article/details/135507131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#怎样训练一个自己的大语言模型？：https://mp.weixin.qq.com/s/F60XYsT1dwUBmXN6r2G0eA\n",
    "#Llama3-8B基于peft+trl进行SFT监督微调（Python代码模式）：https://blog.csdn.net/zhujiahui622/article/details/138196101"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4188358,
     "sourceId": 7233100,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3601853,
     "sourceId": 6266221,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
